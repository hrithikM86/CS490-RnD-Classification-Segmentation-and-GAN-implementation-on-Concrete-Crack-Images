{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from skimage import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "ANaJcQux9UTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxZ3bNkd8aRL",
        "outputId": "77f783d3-f285-4ff2-92bb-5e2b1dc35d31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.6)\n"
          ]
        }
      ],
      "source": [
        "! pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "btZrjftS8kOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "kw8t6cy88mLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "YsPzUYmF8okR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d oluwaseunad/concrete-and-pavement-crack-images"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhY6CvhE8qXv",
        "outputId": "fb630449-0ff7-4336-e56b-3901b2ae3c78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading concrete-and-pavement-crack-images.zip to /content\n",
            "100% 318M/319M [00:17<00:00, 13.9MB/s]\n",
            "100% 319M/319M [00:17<00:00, 19.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q concrete-and-pavement-crack-images.zip"
      ],
      "metadata": {
        "id": "azQbE38M8sHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9pXbXVu8uO3",
        "outputId": "c3c4222b-47ce-4c08-c15b-78eb8b00d385"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "concrete-and-pavement-crack-images.zip\tkaggle.json  Negative  Positive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data paths and labels\n",
        "path_Negative = '/content/Negative'\n",
        "path_Positive = '/content/Positive'\n",
        "positive_images = [os.path.join(path_Positive, img) for img in os.listdir(path_Positive)]\n",
        "negative_images = [os.path.join(path_Negative, img) for img in os.listdir(path_Negative)]\n",
        "all_images = positive_images + negative_images\n",
        "labels = [1] * len(positive_images) + [0] * len(negative_images)  # Convert to integers\n",
        "df = pd.DataFrame(list(zip(all_images, labels)), columns=['Filepath', 'Label'])"
      ],
      "metadata": {
        "id": "pACho-FN8v8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = shuffle(df, random_state=42)"
      ],
      "metadata": {
        "id": "yRzW3AKt9tmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Mpb9SSF-9RPO",
        "outputId": "176c2ba4-d047-43f1-f141-c40c591d066d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                          Filepath  Label\n",
              "2308   /content/Positive/14606.jpg      1\n",
              "22404  /content/Negative/14409.jpg      0\n",
              "23397  /content/Negative/05699.jpg      0\n",
              "25058  /content/Negative/03792.jpg      0\n",
              "2664   /content/Positive/02665.jpg      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-68c16299-0d57-425f-a72d-3be031b51c14\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Filepath</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2308</th>\n",
              "      <td>/content/Positive/14606.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22404</th>\n",
              "      <td>/content/Negative/14409.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23397</th>\n",
              "      <td>/content/Negative/05699.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25058</th>\n",
              "      <td>/content/Negative/03792.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2664</th>\n",
              "      <td>/content/Positive/02665.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-68c16299-0d57-425f-a72d-3be031b51c14')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-68c16299-0d57-425f-a72d-3be031b51c14 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-68c16299-0d57-425f-a72d-3be031b51c14');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c5d8c219-15ff-4396-86dc-16787a739e32\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c5d8c219-15ff-4396-86dc-16787a739e32')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c5d8c219-15ff-4396-86dc-16787a739e32 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 30000,\n  \"fields\": [\n    {\n      \"column\": \"Filepath\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 30000,\n        \"samples\": [\n          \"/content/Positive/08932.jpg\",\n          \"/content/Positive/13231.jpg\",\n          \"/content/Positive/02501.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Label\"].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndyuIY_39SCN",
        "outputId": "0aeda437-9cd9-4626-8b5a-b2a1f921461e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    15000\n",
              "0    15000\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.df.iloc[idx, 0]\n",
        "        image = io.imread(img_name)\n",
        "        label = int(self.df.iloc[idx, 1])\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "OElO15oA9nO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((100, 100)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "metadata": {
        "id": "Zeu3Omlh-XzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from PIL import Image\n",
        "# import torchvision.transforms as transforms\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Define the transformation\n",
        "# transform_ = transforms.Compose([\n",
        "#     transforms.Resize((100, 100)),\n",
        "#     transforms.ToTensor(),\n",
        "# ])\n",
        "\n",
        "# # Function to load and transform the image\n",
        "# def load_and_transform_image(image_path, transform):\n",
        "#     # Load the image\n",
        "#     img = Image.open(image_path)\n",
        "#     # Apply the transformation\n",
        "#     transformed_img = transform(img)\n",
        "#     return transformed_img\n",
        "\n",
        "# image_path = \"/content/Positive/00017.jpg\"\n",
        "\n",
        "# # Load and transform the image\n",
        "# transformed_image = load_and_transform_image(image_path, transform_)\n",
        "\n",
        "# # Convert tensor to numpy array and transpose the dimensions\n",
        "# # to (height, width, channels) for visualization\n",
        "# image_np = transformed_image.numpy().transpose((1, 2, 0))\n",
        "\n",
        "# # Visualize the transformed image\n",
        "# plt.imshow(image_np)\n",
        "# plt.axis('off')\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "tLWKmDGU0GU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into train and test\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "NYZ_q3LT-Zm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(train_df, transform=transform)\n",
        "test_dataset = CustomDataset(test_df, transform=transform)"
      ],
      "metadata": {
        "id": "1QxBpcS1-eJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "testloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "chcwn47R-hqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "class Rasnet50(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Rasnet50, self).__init__()\n",
        "        self.resnet = torchvision.models.resnet50(pretrained=True)\n",
        "        # Freeze ResNet parameters\n",
        "        for param in self.resnet.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.resnet.fc = nn.Sequential(\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.resnet(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "SWuAEop7-lh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "rasnet50 = Rasnet50()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0amJmxN-srl",
        "outputId": "d2aa8371-d7f3-4c5a-df6b-932c7c48ed05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:01<00:00, 55.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rasnet50.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzsUJvKStWlI",
        "outputId": "78d8cc71-08e0-4fc2-f8d7-8950b496c9ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Rasnet50(\n",
              "  (resnet): ResNet(\n",
              "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (layer1): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (4): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (5): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "    (fc): Sequential(\n",
              "      (0): Linear(in_features=2048, out_features=256, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Linear(in_features=256, out_features=128, bias=True)\n",
              "      (3): ReLU()\n",
              "      (4): Linear(in_features=128, out_features=64, bias=True)\n",
              "      (5): ReLU()\n",
              "      (6): Linear(in_features=64, out_features=2, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary(rasnet50,(3,100,100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSNFYnFPtOJO",
        "outputId": "2465717e-7d0e-4ed6-8dd1-30490ff73e08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 50, 50]           9,408\n",
            "       BatchNorm2d-2           [-1, 64, 50, 50]             128\n",
            "              ReLU-3           [-1, 64, 50, 50]               0\n",
            "         MaxPool2d-4           [-1, 64, 25, 25]               0\n",
            "            Conv2d-5           [-1, 64, 25, 25]           4,096\n",
            "       BatchNorm2d-6           [-1, 64, 25, 25]             128\n",
            "              ReLU-7           [-1, 64, 25, 25]               0\n",
            "            Conv2d-8           [-1, 64, 25, 25]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 25, 25]             128\n",
            "             ReLU-10           [-1, 64, 25, 25]               0\n",
            "           Conv2d-11          [-1, 256, 25, 25]          16,384\n",
            "      BatchNorm2d-12          [-1, 256, 25, 25]             512\n",
            "           Conv2d-13          [-1, 256, 25, 25]          16,384\n",
            "      BatchNorm2d-14          [-1, 256, 25, 25]             512\n",
            "             ReLU-15          [-1, 256, 25, 25]               0\n",
            "       Bottleneck-16          [-1, 256, 25, 25]               0\n",
            "           Conv2d-17           [-1, 64, 25, 25]          16,384\n",
            "      BatchNorm2d-18           [-1, 64, 25, 25]             128\n",
            "             ReLU-19           [-1, 64, 25, 25]               0\n",
            "           Conv2d-20           [-1, 64, 25, 25]          36,864\n",
            "      BatchNorm2d-21           [-1, 64, 25, 25]             128\n",
            "             ReLU-22           [-1, 64, 25, 25]               0\n",
            "           Conv2d-23          [-1, 256, 25, 25]          16,384\n",
            "      BatchNorm2d-24          [-1, 256, 25, 25]             512\n",
            "             ReLU-25          [-1, 256, 25, 25]               0\n",
            "       Bottleneck-26          [-1, 256, 25, 25]               0\n",
            "           Conv2d-27           [-1, 64, 25, 25]          16,384\n",
            "      BatchNorm2d-28           [-1, 64, 25, 25]             128\n",
            "             ReLU-29           [-1, 64, 25, 25]               0\n",
            "           Conv2d-30           [-1, 64, 25, 25]          36,864\n",
            "      BatchNorm2d-31           [-1, 64, 25, 25]             128\n",
            "             ReLU-32           [-1, 64, 25, 25]               0\n",
            "           Conv2d-33          [-1, 256, 25, 25]          16,384\n",
            "      BatchNorm2d-34          [-1, 256, 25, 25]             512\n",
            "             ReLU-35          [-1, 256, 25, 25]               0\n",
            "       Bottleneck-36          [-1, 256, 25, 25]               0\n",
            "           Conv2d-37          [-1, 128, 25, 25]          32,768\n",
            "      BatchNorm2d-38          [-1, 128, 25, 25]             256\n",
            "             ReLU-39          [-1, 128, 25, 25]               0\n",
            "           Conv2d-40          [-1, 128, 13, 13]         147,456\n",
            "      BatchNorm2d-41          [-1, 128, 13, 13]             256\n",
            "             ReLU-42          [-1, 128, 13, 13]               0\n",
            "           Conv2d-43          [-1, 512, 13, 13]          65,536\n",
            "      BatchNorm2d-44          [-1, 512, 13, 13]           1,024\n",
            "           Conv2d-45          [-1, 512, 13, 13]         131,072\n",
            "      BatchNorm2d-46          [-1, 512, 13, 13]           1,024\n",
            "             ReLU-47          [-1, 512, 13, 13]               0\n",
            "       Bottleneck-48          [-1, 512, 13, 13]               0\n",
            "           Conv2d-49          [-1, 128, 13, 13]          65,536\n",
            "      BatchNorm2d-50          [-1, 128, 13, 13]             256\n",
            "             ReLU-51          [-1, 128, 13, 13]               0\n",
            "           Conv2d-52          [-1, 128, 13, 13]         147,456\n",
            "      BatchNorm2d-53          [-1, 128, 13, 13]             256\n",
            "             ReLU-54          [-1, 128, 13, 13]               0\n",
            "           Conv2d-55          [-1, 512, 13, 13]          65,536\n",
            "      BatchNorm2d-56          [-1, 512, 13, 13]           1,024\n",
            "             ReLU-57          [-1, 512, 13, 13]               0\n",
            "       Bottleneck-58          [-1, 512, 13, 13]               0\n",
            "           Conv2d-59          [-1, 128, 13, 13]          65,536\n",
            "      BatchNorm2d-60          [-1, 128, 13, 13]             256\n",
            "             ReLU-61          [-1, 128, 13, 13]               0\n",
            "           Conv2d-62          [-1, 128, 13, 13]         147,456\n",
            "      BatchNorm2d-63          [-1, 128, 13, 13]             256\n",
            "             ReLU-64          [-1, 128, 13, 13]               0\n",
            "           Conv2d-65          [-1, 512, 13, 13]          65,536\n",
            "      BatchNorm2d-66          [-1, 512, 13, 13]           1,024\n",
            "             ReLU-67          [-1, 512, 13, 13]               0\n",
            "       Bottleneck-68          [-1, 512, 13, 13]               0\n",
            "           Conv2d-69          [-1, 128, 13, 13]          65,536\n",
            "      BatchNorm2d-70          [-1, 128, 13, 13]             256\n",
            "             ReLU-71          [-1, 128, 13, 13]               0\n",
            "           Conv2d-72          [-1, 128, 13, 13]         147,456\n",
            "      BatchNorm2d-73          [-1, 128, 13, 13]             256\n",
            "             ReLU-74          [-1, 128, 13, 13]               0\n",
            "           Conv2d-75          [-1, 512, 13, 13]          65,536\n",
            "      BatchNorm2d-76          [-1, 512, 13, 13]           1,024\n",
            "             ReLU-77          [-1, 512, 13, 13]               0\n",
            "       Bottleneck-78          [-1, 512, 13, 13]               0\n",
            "           Conv2d-79          [-1, 256, 13, 13]         131,072\n",
            "      BatchNorm2d-80          [-1, 256, 13, 13]             512\n",
            "             ReLU-81          [-1, 256, 13, 13]               0\n",
            "           Conv2d-82            [-1, 256, 7, 7]         589,824\n",
            "      BatchNorm2d-83            [-1, 256, 7, 7]             512\n",
            "             ReLU-84            [-1, 256, 7, 7]               0\n",
            "           Conv2d-85           [-1, 1024, 7, 7]         262,144\n",
            "      BatchNorm2d-86           [-1, 1024, 7, 7]           2,048\n",
            "           Conv2d-87           [-1, 1024, 7, 7]         524,288\n",
            "      BatchNorm2d-88           [-1, 1024, 7, 7]           2,048\n",
            "             ReLU-89           [-1, 1024, 7, 7]               0\n",
            "       Bottleneck-90           [-1, 1024, 7, 7]               0\n",
            "           Conv2d-91            [-1, 256, 7, 7]         262,144\n",
            "      BatchNorm2d-92            [-1, 256, 7, 7]             512\n",
            "             ReLU-93            [-1, 256, 7, 7]               0\n",
            "           Conv2d-94            [-1, 256, 7, 7]         589,824\n",
            "      BatchNorm2d-95            [-1, 256, 7, 7]             512\n",
            "             ReLU-96            [-1, 256, 7, 7]               0\n",
            "           Conv2d-97           [-1, 1024, 7, 7]         262,144\n",
            "      BatchNorm2d-98           [-1, 1024, 7, 7]           2,048\n",
            "             ReLU-99           [-1, 1024, 7, 7]               0\n",
            "      Bottleneck-100           [-1, 1024, 7, 7]               0\n",
            "          Conv2d-101            [-1, 256, 7, 7]         262,144\n",
            "     BatchNorm2d-102            [-1, 256, 7, 7]             512\n",
            "            ReLU-103            [-1, 256, 7, 7]               0\n",
            "          Conv2d-104            [-1, 256, 7, 7]         589,824\n",
            "     BatchNorm2d-105            [-1, 256, 7, 7]             512\n",
            "            ReLU-106            [-1, 256, 7, 7]               0\n",
            "          Conv2d-107           [-1, 1024, 7, 7]         262,144\n",
            "     BatchNorm2d-108           [-1, 1024, 7, 7]           2,048\n",
            "            ReLU-109           [-1, 1024, 7, 7]               0\n",
            "      Bottleneck-110           [-1, 1024, 7, 7]               0\n",
            "          Conv2d-111            [-1, 256, 7, 7]         262,144\n",
            "     BatchNorm2d-112            [-1, 256, 7, 7]             512\n",
            "            ReLU-113            [-1, 256, 7, 7]               0\n",
            "          Conv2d-114            [-1, 256, 7, 7]         589,824\n",
            "     BatchNorm2d-115            [-1, 256, 7, 7]             512\n",
            "            ReLU-116            [-1, 256, 7, 7]               0\n",
            "          Conv2d-117           [-1, 1024, 7, 7]         262,144\n",
            "     BatchNorm2d-118           [-1, 1024, 7, 7]           2,048\n",
            "            ReLU-119           [-1, 1024, 7, 7]               0\n",
            "      Bottleneck-120           [-1, 1024, 7, 7]               0\n",
            "          Conv2d-121            [-1, 256, 7, 7]         262,144\n",
            "     BatchNorm2d-122            [-1, 256, 7, 7]             512\n",
            "            ReLU-123            [-1, 256, 7, 7]               0\n",
            "          Conv2d-124            [-1, 256, 7, 7]         589,824\n",
            "     BatchNorm2d-125            [-1, 256, 7, 7]             512\n",
            "            ReLU-126            [-1, 256, 7, 7]               0\n",
            "          Conv2d-127           [-1, 1024, 7, 7]         262,144\n",
            "     BatchNorm2d-128           [-1, 1024, 7, 7]           2,048\n",
            "            ReLU-129           [-1, 1024, 7, 7]               0\n",
            "      Bottleneck-130           [-1, 1024, 7, 7]               0\n",
            "          Conv2d-131            [-1, 256, 7, 7]         262,144\n",
            "     BatchNorm2d-132            [-1, 256, 7, 7]             512\n",
            "            ReLU-133            [-1, 256, 7, 7]               0\n",
            "          Conv2d-134            [-1, 256, 7, 7]         589,824\n",
            "     BatchNorm2d-135            [-1, 256, 7, 7]             512\n",
            "            ReLU-136            [-1, 256, 7, 7]               0\n",
            "          Conv2d-137           [-1, 1024, 7, 7]         262,144\n",
            "     BatchNorm2d-138           [-1, 1024, 7, 7]           2,048\n",
            "            ReLU-139           [-1, 1024, 7, 7]               0\n",
            "      Bottleneck-140           [-1, 1024, 7, 7]               0\n",
            "          Conv2d-141            [-1, 512, 7, 7]         524,288\n",
            "     BatchNorm2d-142            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-143            [-1, 512, 7, 7]               0\n",
            "          Conv2d-144            [-1, 512, 4, 4]       2,359,296\n",
            "     BatchNorm2d-145            [-1, 512, 4, 4]           1,024\n",
            "            ReLU-146            [-1, 512, 4, 4]               0\n",
            "          Conv2d-147           [-1, 2048, 4, 4]       1,048,576\n",
            "     BatchNorm2d-148           [-1, 2048, 4, 4]           4,096\n",
            "          Conv2d-149           [-1, 2048, 4, 4]       2,097,152\n",
            "     BatchNorm2d-150           [-1, 2048, 4, 4]           4,096\n",
            "            ReLU-151           [-1, 2048, 4, 4]               0\n",
            "      Bottleneck-152           [-1, 2048, 4, 4]               0\n",
            "          Conv2d-153            [-1, 512, 4, 4]       1,048,576\n",
            "     BatchNorm2d-154            [-1, 512, 4, 4]           1,024\n",
            "            ReLU-155            [-1, 512, 4, 4]               0\n",
            "          Conv2d-156            [-1, 512, 4, 4]       2,359,296\n",
            "     BatchNorm2d-157            [-1, 512, 4, 4]           1,024\n",
            "            ReLU-158            [-1, 512, 4, 4]               0\n",
            "          Conv2d-159           [-1, 2048, 4, 4]       1,048,576\n",
            "     BatchNorm2d-160           [-1, 2048, 4, 4]           4,096\n",
            "            ReLU-161           [-1, 2048, 4, 4]               0\n",
            "      Bottleneck-162           [-1, 2048, 4, 4]               0\n",
            "          Conv2d-163            [-1, 512, 4, 4]       1,048,576\n",
            "     BatchNorm2d-164            [-1, 512, 4, 4]           1,024\n",
            "            ReLU-165            [-1, 512, 4, 4]               0\n",
            "          Conv2d-166            [-1, 512, 4, 4]       2,359,296\n",
            "     BatchNorm2d-167            [-1, 512, 4, 4]           1,024\n",
            "            ReLU-168            [-1, 512, 4, 4]               0\n",
            "          Conv2d-169           [-1, 2048, 4, 4]       1,048,576\n",
            "     BatchNorm2d-170           [-1, 2048, 4, 4]           4,096\n",
            "            ReLU-171           [-1, 2048, 4, 4]               0\n",
            "      Bottleneck-172           [-1, 2048, 4, 4]               0\n",
            "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
            "          Linear-174                  [-1, 256]         524,544\n",
            "            ReLU-175                  [-1, 256]               0\n",
            "          Linear-176                  [-1, 128]          32,896\n",
            "            ReLU-177                  [-1, 128]               0\n",
            "          Linear-178                   [-1, 64]           8,256\n",
            "            ReLU-179                   [-1, 64]               0\n",
            "          Linear-180                    [-1, 2]             130\n",
            "          ResNet-181                    [-1, 2]               0\n",
            "================================================================\n",
            "Total params: 24,073,858\n",
            "Trainable params: 565,826\n",
            "Non-trainable params: 23,508,032\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.11\n",
            "Forward/backward pass size (MB): 62.87\n",
            "Params size (MB): 91.83\n",
            "Estimated Total Size (MB): 154.81\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(rasnet50.resnet.fc.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "0tDKSb-G_DuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "no_of_epoch = 10\n",
        "for epoch in range(no_of_epoch):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = rasnet50(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 10 == 9:  # print every 10 mini-batches\n",
        "            print(f\"Epoch [{epoch + 1}/{no_of_epoch}], Batch [{i + 1}/{len(trainloader)}], Loss: {running_loss / 10:.3f} (epoch loss)\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Accuracy of the network on the {total} train images: %d %%' % (\n",
        "        100 * correct / total))\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2_EsYhQ_GOk",
        "outputId": "e11014c8-c749-4f73-fab1-8a16927ad601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Batch [10/375], Loss: 0.557 (epoch loss)\n",
            "Epoch [1/10], Batch [20/375], Loss: 0.263 (epoch loss)\n",
            "Epoch [1/10], Batch [30/375], Loss: 0.176 (epoch loss)\n",
            "Epoch [1/10], Batch [40/375], Loss: 0.120 (epoch loss)\n",
            "Epoch [1/10], Batch [50/375], Loss: 0.103 (epoch loss)\n",
            "Epoch [1/10], Batch [60/375], Loss: 0.103 (epoch loss)\n",
            "Epoch [1/10], Batch [70/375], Loss: 0.226 (epoch loss)\n",
            "Epoch [1/10], Batch [80/375], Loss: 0.173 (epoch loss)\n",
            "Epoch [1/10], Batch [90/375], Loss: 0.127 (epoch loss)\n",
            "Epoch [1/10], Batch [100/375], Loss: 0.142 (epoch loss)\n",
            "Epoch [1/10], Batch [110/375], Loss: 0.151 (epoch loss)\n",
            "Epoch [1/10], Batch [120/375], Loss: 0.163 (epoch loss)\n",
            "Epoch [1/10], Batch [130/375], Loss: 0.228 (epoch loss)\n",
            "Epoch [1/10], Batch [140/375], Loss: 0.115 (epoch loss)\n",
            "Epoch [1/10], Batch [150/375], Loss: 0.125 (epoch loss)\n",
            "Epoch [1/10], Batch [160/375], Loss: 0.105 (epoch loss)\n",
            "Epoch [1/10], Batch [170/375], Loss: 0.107 (epoch loss)\n",
            "Epoch [1/10], Batch [180/375], Loss: 0.068 (epoch loss)\n",
            "Epoch [1/10], Batch [190/375], Loss: 0.082 (epoch loss)\n",
            "Epoch [1/10], Batch [200/375], Loss: 0.072 (epoch loss)\n",
            "Epoch [1/10], Batch [210/375], Loss: 0.067 (epoch loss)\n",
            "Epoch [1/10], Batch [220/375], Loss: 0.088 (epoch loss)\n",
            "Epoch [1/10], Batch [230/375], Loss: 0.080 (epoch loss)\n",
            "Epoch [1/10], Batch [240/375], Loss: 0.103 (epoch loss)\n",
            "Epoch [1/10], Batch [250/375], Loss: 0.092 (epoch loss)\n",
            "Epoch [1/10], Batch [260/375], Loss: 0.103 (epoch loss)\n",
            "Epoch [1/10], Batch [270/375], Loss: 0.062 (epoch loss)\n",
            "Epoch [1/10], Batch [280/375], Loss: 0.094 (epoch loss)\n",
            "Epoch [1/10], Batch [290/375], Loss: 0.066 (epoch loss)\n",
            "Epoch [1/10], Batch [300/375], Loss: 0.070 (epoch loss)\n",
            "Epoch [1/10], Batch [310/375], Loss: 0.068 (epoch loss)\n",
            "Epoch [1/10], Batch [320/375], Loss: 0.084 (epoch loss)\n",
            "Epoch [1/10], Batch [330/375], Loss: 0.093 (epoch loss)\n",
            "Epoch [1/10], Batch [340/375], Loss: 0.091 (epoch loss)\n",
            "Epoch [1/10], Batch [350/375], Loss: 0.065 (epoch loss)\n",
            "Epoch [1/10], Batch [360/375], Loss: 0.084 (epoch loss)\n",
            "Epoch [1/10], Batch [370/375], Loss: 0.053 (epoch loss)\n",
            "Accuracy of the network on the 24000 train images: 95 %\n",
            "Epoch [2/10], Batch [10/375], Loss: 0.080 (epoch loss)\n",
            "Epoch [2/10], Batch [20/375], Loss: 0.054 (epoch loss)\n",
            "Epoch [2/10], Batch [30/375], Loss: 0.093 (epoch loss)\n",
            "Epoch [2/10], Batch [40/375], Loss: 0.131 (epoch loss)\n",
            "Epoch [2/10], Batch [50/375], Loss: 0.136 (epoch loss)\n",
            "Epoch [2/10], Batch [60/375], Loss: 0.093 (epoch loss)\n",
            "Epoch [2/10], Batch [70/375], Loss: 0.056 (epoch loss)\n",
            "Epoch [2/10], Batch [80/375], Loss: 0.026 (epoch loss)\n",
            "Epoch [2/10], Batch [90/375], Loss: 0.088 (epoch loss)\n",
            "Epoch [2/10], Batch [100/375], Loss: 0.048 (epoch loss)\n",
            "Epoch [2/10], Batch [110/375], Loss: 0.068 (epoch loss)\n",
            "Epoch [2/10], Batch [120/375], Loss: 0.041 (epoch loss)\n",
            "Epoch [2/10], Batch [130/375], Loss: 0.038 (epoch loss)\n",
            "Epoch [2/10], Batch [140/375], Loss: 0.064 (epoch loss)\n",
            "Epoch [2/10], Batch [150/375], Loss: 0.043 (epoch loss)\n",
            "Epoch [2/10], Batch [160/375], Loss: 0.045 (epoch loss)\n",
            "Epoch [2/10], Batch [170/375], Loss: 0.056 (epoch loss)\n",
            "Epoch [2/10], Batch [180/375], Loss: 0.070 (epoch loss)\n",
            "Epoch [2/10], Batch [190/375], Loss: 0.078 (epoch loss)\n",
            "Epoch [2/10], Batch [200/375], Loss: 0.109 (epoch loss)\n",
            "Epoch [2/10], Batch [210/375], Loss: 0.070 (epoch loss)\n",
            "Epoch [2/10], Batch [220/375], Loss: 0.068 (epoch loss)\n",
            "Epoch [2/10], Batch [230/375], Loss: 0.056 (epoch loss)\n",
            "Epoch [2/10], Batch [240/375], Loss: 0.076 (epoch loss)\n",
            "Epoch [2/10], Batch [250/375], Loss: 0.053 (epoch loss)\n",
            "Epoch [2/10], Batch [260/375], Loss: 0.044 (epoch loss)\n",
            "Epoch [2/10], Batch [270/375], Loss: 0.094 (epoch loss)\n",
            "Epoch [2/10], Batch [280/375], Loss: 0.038 (epoch loss)\n",
            "Epoch [2/10], Batch [290/375], Loss: 0.071 (epoch loss)\n",
            "Epoch [2/10], Batch [300/375], Loss: 0.065 (epoch loss)\n",
            "Epoch [2/10], Batch [310/375], Loss: 0.086 (epoch loss)\n",
            "Epoch [2/10], Batch [320/375], Loss: 0.032 (epoch loss)\n",
            "Epoch [2/10], Batch [330/375], Loss: 0.068 (epoch loss)\n",
            "Epoch [2/10], Batch [340/375], Loss: 0.066 (epoch loss)\n",
            "Epoch [2/10], Batch [350/375], Loss: 0.060 (epoch loss)\n",
            "Epoch [2/10], Batch [360/375], Loss: 0.068 (epoch loss)\n",
            "Epoch [2/10], Batch [370/375], Loss: 0.036 (epoch loss)\n",
            "Accuracy of the network on the 24000 train images: 97 %\n",
            "Epoch [3/10], Batch [10/375], Loss: 0.066 (epoch loss)\n",
            "Epoch [3/10], Batch [20/375], Loss: 0.041 (epoch loss)\n",
            "Epoch [3/10], Batch [30/375], Loss: 0.054 (epoch loss)\n",
            "Epoch [3/10], Batch [40/375], Loss: 0.045 (epoch loss)\n",
            "Epoch [3/10], Batch [50/375], Loss: 0.030 (epoch loss)\n",
            "Epoch [3/10], Batch [60/375], Loss: 0.020 (epoch loss)\n",
            "Epoch [3/10], Batch [70/375], Loss: 0.034 (epoch loss)\n",
            "Epoch [3/10], Batch [80/375], Loss: 0.057 (epoch loss)\n",
            "Epoch [3/10], Batch [90/375], Loss: 0.061 (epoch loss)\n",
            "Epoch [3/10], Batch [100/375], Loss: 0.038 (epoch loss)\n",
            "Epoch [3/10], Batch [110/375], Loss: 0.047 (epoch loss)\n",
            "Epoch [3/10], Batch [120/375], Loss: 0.084 (epoch loss)\n",
            "Epoch [3/10], Batch [130/375], Loss: 0.049 (epoch loss)\n",
            "Epoch [3/10], Batch [140/375], Loss: 0.069 (epoch loss)\n",
            "Epoch [3/10], Batch [150/375], Loss: 0.052 (epoch loss)\n",
            "Epoch [3/10], Batch [160/375], Loss: 0.034 (epoch loss)\n",
            "Epoch [3/10], Batch [170/375], Loss: 0.042 (epoch loss)\n",
            "Epoch [3/10], Batch [180/375], Loss: 0.110 (epoch loss)\n",
            "Epoch [3/10], Batch [190/375], Loss: 0.104 (epoch loss)\n",
            "Epoch [3/10], Batch [200/375], Loss: 0.062 (epoch loss)\n",
            "Epoch [3/10], Batch [210/375], Loss: 0.064 (epoch loss)\n",
            "Epoch [3/10], Batch [220/375], Loss: 0.068 (epoch loss)\n",
            "Epoch [3/10], Batch [230/375], Loss: 0.040 (epoch loss)\n",
            "Epoch [3/10], Batch [240/375], Loss: 0.032 (epoch loss)\n",
            "Epoch [3/10], Batch [250/375], Loss: 0.048 (epoch loss)\n",
            "Epoch [3/10], Batch [260/375], Loss: 0.055 (epoch loss)\n",
            "Epoch [3/10], Batch [270/375], Loss: 0.058 (epoch loss)\n",
            "Epoch [3/10], Batch [280/375], Loss: 0.081 (epoch loss)\n",
            "Epoch [3/10], Batch [290/375], Loss: 0.062 (epoch loss)\n",
            "Epoch [3/10], Batch [300/375], Loss: 0.048 (epoch loss)\n",
            "Epoch [3/10], Batch [310/375], Loss: 0.042 (epoch loss)\n",
            "Epoch [3/10], Batch [320/375], Loss: 0.054 (epoch loss)\n",
            "Epoch [3/10], Batch [330/375], Loss: 0.033 (epoch loss)\n",
            "Epoch [3/10], Batch [340/375], Loss: 0.049 (epoch loss)\n",
            "Epoch [3/10], Batch [350/375], Loss: 0.036 (epoch loss)\n",
            "Epoch [3/10], Batch [360/375], Loss: 0.034 (epoch loss)\n",
            "Epoch [3/10], Batch [370/375], Loss: 0.036 (epoch loss)\n",
            "Accuracy of the network on the 24000 train images: 98 %\n",
            "Epoch [4/10], Batch [10/375], Loss: 0.035 (epoch loss)\n",
            "Epoch [4/10], Batch [20/375], Loss: 0.030 (epoch loss)\n",
            "Epoch [4/10], Batch [30/375], Loss: 0.064 (epoch loss)\n",
            "Epoch [4/10], Batch [40/375], Loss: 0.067 (epoch loss)\n",
            "Epoch [4/10], Batch [50/375], Loss: 0.077 (epoch loss)\n",
            "Epoch [4/10], Batch [60/375], Loss: 0.044 (epoch loss)\n",
            "Epoch [4/10], Batch [70/375], Loss: 0.024 (epoch loss)\n",
            "Epoch [4/10], Batch [80/375], Loss: 0.055 (epoch loss)\n",
            "Epoch [4/10], Batch [90/375], Loss: 0.043 (epoch loss)\n",
            "Epoch [4/10], Batch [100/375], Loss: 0.064 (epoch loss)\n",
            "Epoch [4/10], Batch [110/375], Loss: 0.076 (epoch loss)\n",
            "Epoch [4/10], Batch [120/375], Loss: 0.057 (epoch loss)\n",
            "Epoch [4/10], Batch [130/375], Loss: 0.032 (epoch loss)\n",
            "Epoch [4/10], Batch [140/375], Loss: 0.040 (epoch loss)\n",
            "Epoch [4/10], Batch [150/375], Loss: 0.044 (epoch loss)\n",
            "Epoch [4/10], Batch [160/375], Loss: 0.048 (epoch loss)\n",
            "Epoch [4/10], Batch [170/375], Loss: 0.050 (epoch loss)\n",
            "Epoch [4/10], Batch [180/375], Loss: 0.067 (epoch loss)\n",
            "Epoch [4/10], Batch [190/375], Loss: 0.100 (epoch loss)\n",
            "Epoch [4/10], Batch [200/375], Loss: 0.058 (epoch loss)\n",
            "Epoch [4/10], Batch [210/375], Loss: 0.043 (epoch loss)\n",
            "Epoch [4/10], Batch [220/375], Loss: 0.026 (epoch loss)\n",
            "Epoch [4/10], Batch [230/375], Loss: 0.054 (epoch loss)\n",
            "Epoch [4/10], Batch [240/375], Loss: 0.052 (epoch loss)\n",
            "Epoch [4/10], Batch [250/375], Loss: 0.061 (epoch loss)\n",
            "Epoch [4/10], Batch [260/375], Loss: 0.067 (epoch loss)\n",
            "Epoch [4/10], Batch [270/375], Loss: 0.047 (epoch loss)\n",
            "Epoch [4/10], Batch [280/375], Loss: 0.044 (epoch loss)\n",
            "Epoch [4/10], Batch [290/375], Loss: 0.025 (epoch loss)\n",
            "Epoch [4/10], Batch [300/375], Loss: 0.030 (epoch loss)\n",
            "Epoch [4/10], Batch [310/375], Loss: 0.091 (epoch loss)\n",
            "Epoch [4/10], Batch [320/375], Loss: 0.049 (epoch loss)\n",
            "Epoch [4/10], Batch [330/375], Loss: 0.031 (epoch loss)\n",
            "Epoch [4/10], Batch [340/375], Loss: 0.022 (epoch loss)\n",
            "Epoch [4/10], Batch [350/375], Loss: 0.044 (epoch loss)\n",
            "Epoch [4/10], Batch [360/375], Loss: 0.026 (epoch loss)\n",
            "Epoch [4/10], Batch [370/375], Loss: 0.028 (epoch loss)\n",
            "Accuracy of the network on the 24000 train images: 98 %\n",
            "Epoch [5/10], Batch [10/375], Loss: 0.046 (epoch loss)\n",
            "Epoch [5/10], Batch [20/375], Loss: 0.035 (epoch loss)\n",
            "Epoch [5/10], Batch [30/375], Loss: 0.025 (epoch loss)\n",
            "Epoch [5/10], Batch [40/375], Loss: 0.056 (epoch loss)\n",
            "Epoch [5/10], Batch [50/375], Loss: 0.049 (epoch loss)\n",
            "Epoch [5/10], Batch [60/375], Loss: 0.027 (epoch loss)\n",
            "Epoch [5/10], Batch [70/375], Loss: 0.026 (epoch loss)\n",
            "Epoch [5/10], Batch [80/375], Loss: 0.069 (epoch loss)\n",
            "Epoch [5/10], Batch [90/375], Loss: 0.025 (epoch loss)\n",
            "Epoch [5/10], Batch [100/375], Loss: 0.020 (epoch loss)\n",
            "Epoch [5/10], Batch [110/375], Loss: 0.034 (epoch loss)\n",
            "Epoch [5/10], Batch [120/375], Loss: 0.030 (epoch loss)\n",
            "Epoch [5/10], Batch [130/375], Loss: 0.025 (epoch loss)\n",
            "Epoch [5/10], Batch [140/375], Loss: 0.019 (epoch loss)\n",
            "Epoch [5/10], Batch [150/375], Loss: 0.016 (epoch loss)\n",
            "Epoch [5/10], Batch [160/375], Loss: 0.036 (epoch loss)\n",
            "Epoch [5/10], Batch [170/375], Loss: 0.030 (epoch loss)\n",
            "Epoch [5/10], Batch [180/375], Loss: 0.037 (epoch loss)\n",
            "Epoch [5/10], Batch [190/375], Loss: 0.028 (epoch loss)\n",
            "Epoch [5/10], Batch [200/375], Loss: 0.024 (epoch loss)\n",
            "Epoch [5/10], Batch [210/375], Loss: 0.024 (epoch loss)\n",
            "Epoch [5/10], Batch [220/375], Loss: 0.034 (epoch loss)\n",
            "Epoch [5/10], Batch [230/375], Loss: 0.020 (epoch loss)\n",
            "Epoch [5/10], Batch [240/375], Loss: 0.035 (epoch loss)\n",
            "Epoch [5/10], Batch [250/375], Loss: 0.014 (epoch loss)\n",
            "Epoch [5/10], Batch [260/375], Loss: 0.023 (epoch loss)\n",
            "Epoch [5/10], Batch [270/375], Loss: 0.027 (epoch loss)\n",
            "Epoch [5/10], Batch [280/375], Loss: 0.017 (epoch loss)\n",
            "Epoch [5/10], Batch [290/375], Loss: 0.042 (epoch loss)\n",
            "Epoch [5/10], Batch [300/375], Loss: 0.048 (epoch loss)\n",
            "Epoch [5/10], Batch [310/375], Loss: 0.058 (epoch loss)\n",
            "Epoch [5/10], Batch [320/375], Loss: 0.033 (epoch loss)\n",
            "Epoch [5/10], Batch [330/375], Loss: 0.033 (epoch loss)\n",
            "Epoch [5/10], Batch [340/375], Loss: 0.024 (epoch loss)\n",
            "Epoch [5/10], Batch [350/375], Loss: 0.026 (epoch loss)\n",
            "Epoch [5/10], Batch [360/375], Loss: 0.045 (epoch loss)\n",
            "Epoch [5/10], Batch [370/375], Loss: 0.041 (epoch loss)\n",
            "Accuracy of the network on the 24000 train images: 98 %\n",
            "Epoch [6/10], Batch [10/375], Loss: 0.046 (epoch loss)\n",
            "Epoch [6/10], Batch [20/375], Loss: 0.034 (epoch loss)\n",
            "Epoch [6/10], Batch [30/375], Loss: 0.059 (epoch loss)\n",
            "Epoch [6/10], Batch [40/375], Loss: 0.035 (epoch loss)\n",
            "Epoch [6/10], Batch [50/375], Loss: 0.022 (epoch loss)\n",
            "Epoch [6/10], Batch [60/375], Loss: 0.020 (epoch loss)\n",
            "Epoch [6/10], Batch [70/375], Loss: 0.035 (epoch loss)\n",
            "Epoch [6/10], Batch [80/375], Loss: 0.025 (epoch loss)\n",
            "Epoch [6/10], Batch [90/375], Loss: 0.039 (epoch loss)\n",
            "Epoch [6/10], Batch [100/375], Loss: 0.044 (epoch loss)\n",
            "Epoch [6/10], Batch [110/375], Loss: 0.048 (epoch loss)\n",
            "Epoch [6/10], Batch [120/375], Loss: 0.041 (epoch loss)\n",
            "Epoch [6/10], Batch [130/375], Loss: 0.031 (epoch loss)\n",
            "Epoch [6/10], Batch [140/375], Loss: 0.027 (epoch loss)\n",
            "Epoch [6/10], Batch [150/375], Loss: 0.028 (epoch loss)\n",
            "Epoch [6/10], Batch [160/375], Loss: 0.028 (epoch loss)\n",
            "Epoch [6/10], Batch [170/375], Loss: 0.028 (epoch loss)\n",
            "Epoch [6/10], Batch [180/375], Loss: 0.021 (epoch loss)\n",
            "Epoch [6/10], Batch [190/375], Loss: 0.030 (epoch loss)\n",
            "Epoch [6/10], Batch [200/375], Loss: 0.028 (epoch loss)\n",
            "Epoch [6/10], Batch [210/375], Loss: 0.025 (epoch loss)\n",
            "Epoch [6/10], Batch [220/375], Loss: 0.042 (epoch loss)\n",
            "Epoch [6/10], Batch [230/375], Loss: 0.031 (epoch loss)\n",
            "Epoch [6/10], Batch [240/375], Loss: 0.036 (epoch loss)\n",
            "Epoch [6/10], Batch [250/375], Loss: 0.044 (epoch loss)\n",
            "Epoch [6/10], Batch [260/375], Loss: 0.033 (epoch loss)\n",
            "Epoch [6/10], Batch [270/375], Loss: 0.025 (epoch loss)\n",
            "Epoch [6/10], Batch [280/375], Loss: 0.021 (epoch loss)\n",
            "Epoch [6/10], Batch [290/375], Loss: 0.014 (epoch loss)\n",
            "Epoch [6/10], Batch [300/375], Loss: 0.027 (epoch loss)\n",
            "Epoch [6/10], Batch [310/375], Loss: 0.026 (epoch loss)\n",
            "Epoch [6/10], Batch [320/375], Loss: 0.014 (epoch loss)\n",
            "Epoch [6/10], Batch [330/375], Loss: 0.038 (epoch loss)\n",
            "Epoch [6/10], Batch [340/375], Loss: 0.025 (epoch loss)\n",
            "Epoch [6/10], Batch [350/375], Loss: 0.020 (epoch loss)\n",
            "Epoch [6/10], Batch [360/375], Loss: 0.018 (epoch loss)\n",
            "Epoch [6/10], Batch [370/375], Loss: 0.027 (epoch loss)\n",
            "Accuracy of the network on the 24000 train images: 98 %\n",
            "Epoch [7/10], Batch [10/375], Loss: 0.021 (epoch loss)\n",
            "Epoch [7/10], Batch [20/375], Loss: 0.014 (epoch loss)\n",
            "Epoch [7/10], Batch [30/375], Loss: 0.020 (epoch loss)\n",
            "Epoch [7/10], Batch [40/375], Loss: 0.041 (epoch loss)\n",
            "Epoch [7/10], Batch [50/375], Loss: 0.035 (epoch loss)\n",
            "Epoch [7/10], Batch [60/375], Loss: 0.048 (epoch loss)\n",
            "Epoch [7/10], Batch [70/375], Loss: 0.021 (epoch loss)\n",
            "Epoch [7/10], Batch [80/375], Loss: 0.022 (epoch loss)\n",
            "Epoch [7/10], Batch [90/375], Loss: 0.007 (epoch loss)\n",
            "Epoch [7/10], Batch [100/375], Loss: 0.033 (epoch loss)\n",
            "Epoch [7/10], Batch [110/375], Loss: 0.032 (epoch loss)\n",
            "Epoch [7/10], Batch [120/375], Loss: 0.016 (epoch loss)\n",
            "Epoch [7/10], Batch [130/375], Loss: 0.028 (epoch loss)\n",
            "Epoch [7/10], Batch [140/375], Loss: 0.011 (epoch loss)\n",
            "Epoch [7/10], Batch [150/375], Loss: 0.014 (epoch loss)\n",
            "Epoch [7/10], Batch [160/375], Loss: 0.018 (epoch loss)\n",
            "Epoch [7/10], Batch [170/375], Loss: 0.031 (epoch loss)\n",
            "Epoch [7/10], Batch [180/375], Loss: 0.009 (epoch loss)\n",
            "Epoch [7/10], Batch [190/375], Loss: 0.023 (epoch loss)\n",
            "Epoch [7/10], Batch [200/375], Loss: 0.031 (epoch loss)\n",
            "Epoch [7/10], Batch [210/375], Loss: 0.010 (epoch loss)\n",
            "Epoch [7/10], Batch [220/375], Loss: 0.021 (epoch loss)\n",
            "Epoch [7/10], Batch [230/375], Loss: 0.023 (epoch loss)\n",
            "Epoch [7/10], Batch [240/375], Loss: 0.029 (epoch loss)\n",
            "Epoch [7/10], Batch [250/375], Loss: 0.015 (epoch loss)\n",
            "Epoch [7/10], Batch [260/375], Loss: 0.015 (epoch loss)\n",
            "Epoch [7/10], Batch [270/375], Loss: 0.018 (epoch loss)\n",
            "Epoch [7/10], Batch [280/375], Loss: 0.015 (epoch loss)\n",
            "Epoch [7/10], Batch [290/375], Loss: 0.052 (epoch loss)\n",
            "Epoch [7/10], Batch [300/375], Loss: 0.027 (epoch loss)\n",
            "Epoch [7/10], Batch [310/375], Loss: 0.020 (epoch loss)\n",
            "Epoch [7/10], Batch [320/375], Loss: 0.014 (epoch loss)\n",
            "Epoch [7/10], Batch [330/375], Loss: 0.026 (epoch loss)\n",
            "Epoch [7/10], Batch [340/375], Loss: 0.017 (epoch loss)\n",
            "Epoch [7/10], Batch [350/375], Loss: 0.022 (epoch loss)\n",
            "Epoch [7/10], Batch [360/375], Loss: 0.014 (epoch loss)\n",
            "Epoch [7/10], Batch [370/375], Loss: 0.059 (epoch loss)\n",
            "Accuracy of the network on the 24000 train images: 99 %\n",
            "Epoch [8/10], Batch [10/375], Loss: 0.033 (epoch loss)\n",
            "Epoch [8/10], Batch [20/375], Loss: 0.022 (epoch loss)\n",
            "Epoch [8/10], Batch [30/375], Loss: 0.018 (epoch loss)\n",
            "Epoch [8/10], Batch [40/375], Loss: 0.030 (epoch loss)\n",
            "Epoch [8/10], Batch [50/375], Loss: 0.021 (epoch loss)\n",
            "Epoch [8/10], Batch [60/375], Loss: 0.006 (epoch loss)\n",
            "Epoch [8/10], Batch [70/375], Loss: 0.010 (epoch loss)\n",
            "Epoch [8/10], Batch [80/375], Loss: 0.026 (epoch loss)\n",
            "Epoch [8/10], Batch [90/375], Loss: 0.022 (epoch loss)\n",
            "Epoch [8/10], Batch [100/375], Loss: 0.020 (epoch loss)\n",
            "Epoch [8/10], Batch [110/375], Loss: 0.015 (epoch loss)\n",
            "Epoch [8/10], Batch [120/375], Loss: 0.014 (epoch loss)\n",
            "Epoch [8/10], Batch [130/375], Loss: 0.016 (epoch loss)\n",
            "Epoch [8/10], Batch [140/375], Loss: 0.025 (epoch loss)\n",
            "Epoch [8/10], Batch [150/375], Loss: 0.017 (epoch loss)\n",
            "Epoch [8/10], Batch [160/375], Loss: 0.017 (epoch loss)\n",
            "Epoch [8/10], Batch [170/375], Loss: 0.028 (epoch loss)\n",
            "Epoch [8/10], Batch [180/375], Loss: 0.014 (epoch loss)\n",
            "Epoch [8/10], Batch [190/375], Loss: 0.020 (epoch loss)\n",
            "Epoch [8/10], Batch [200/375], Loss: 0.021 (epoch loss)\n",
            "Epoch [8/10], Batch [210/375], Loss: 0.005 (epoch loss)\n",
            "Epoch [8/10], Batch [220/375], Loss: 0.015 (epoch loss)\n",
            "Epoch [8/10], Batch [230/375], Loss: 0.018 (epoch loss)\n",
            "Epoch [8/10], Batch [240/375], Loss: 0.050 (epoch loss)\n",
            "Epoch [8/10], Batch [250/375], Loss: 0.031 (epoch loss)\n",
            "Epoch [8/10], Batch [260/375], Loss: 0.068 (epoch loss)\n",
            "Epoch [8/10], Batch [270/375], Loss: 0.065 (epoch loss)\n",
            "Epoch [8/10], Batch [280/375], Loss: 0.034 (epoch loss)\n",
            "Epoch [8/10], Batch [290/375], Loss: 0.016 (epoch loss)\n",
            "Epoch [8/10], Batch [300/375], Loss: 0.031 (epoch loss)\n",
            "Epoch [8/10], Batch [310/375], Loss: 0.025 (epoch loss)\n",
            "Epoch [8/10], Batch [320/375], Loss: 0.025 (epoch loss)\n",
            "Epoch [8/10], Batch [330/375], Loss: 0.028 (epoch loss)\n",
            "Epoch [8/10], Batch [340/375], Loss: 0.036 (epoch loss)\n",
            "Epoch [8/10], Batch [350/375], Loss: 0.041 (epoch loss)\n",
            "Epoch [8/10], Batch [360/375], Loss: 0.026 (epoch loss)\n",
            "Epoch [8/10], Batch [370/375], Loss: 0.021 (epoch loss)\n",
            "Accuracy of the network on the 24000 train images: 99 %\n",
            "Epoch [9/10], Batch [10/375], Loss: 0.025 (epoch loss)\n",
            "Epoch [9/10], Batch [20/375], Loss: 0.040 (epoch loss)\n",
            "Epoch [9/10], Batch [30/375], Loss: 0.035 (epoch loss)\n",
            "Epoch [9/10], Batch [40/375], Loss: 0.021 (epoch loss)\n",
            "Epoch [9/10], Batch [50/375], Loss: 0.016 (epoch loss)\n",
            "Epoch [9/10], Batch [60/375], Loss: 0.011 (epoch loss)\n",
            "Epoch [9/10], Batch [70/375], Loss: 0.011 (epoch loss)\n",
            "Epoch [9/10], Batch [80/375], Loss: 0.007 (epoch loss)\n",
            "Epoch [9/10], Batch [90/375], Loss: 0.022 (epoch loss)\n",
            "Epoch [9/10], Batch [100/375], Loss: 0.015 (epoch loss)\n",
            "Epoch [9/10], Batch [110/375], Loss: 0.023 (epoch loss)\n",
            "Epoch [9/10], Batch [120/375], Loss: 0.013 (epoch loss)\n",
            "Epoch [9/10], Batch [130/375], Loss: 0.024 (epoch loss)\n",
            "Epoch [9/10], Batch [140/375], Loss: 0.033 (epoch loss)\n",
            "Epoch [9/10], Batch [150/375], Loss: 0.014 (epoch loss)\n",
            "Epoch [9/10], Batch [160/375], Loss: 0.018 (epoch loss)\n",
            "Epoch [9/10], Batch [170/375], Loss: 0.034 (epoch loss)\n",
            "Epoch [9/10], Batch [180/375], Loss: 0.026 (epoch loss)\n",
            "Epoch [9/10], Batch [190/375], Loss: 0.025 (epoch loss)\n",
            "Epoch [9/10], Batch [200/375], Loss: 0.038 (epoch loss)\n",
            "Epoch [9/10], Batch [210/375], Loss: 0.010 (epoch loss)\n",
            "Epoch [9/10], Batch [220/375], Loss: 0.005 (epoch loss)\n",
            "Epoch [9/10], Batch [230/375], Loss: 0.016 (epoch loss)\n",
            "Epoch [9/10], Batch [240/375], Loss: 0.018 (epoch loss)\n",
            "Epoch [9/10], Batch [250/375], Loss: 0.019 (epoch loss)\n",
            "Epoch [9/10], Batch [260/375], Loss: 0.031 (epoch loss)\n",
            "Epoch [9/10], Batch [270/375], Loss: 0.023 (epoch loss)\n",
            "Epoch [9/10], Batch [280/375], Loss: 0.018 (epoch loss)\n",
            "Epoch [9/10], Batch [290/375], Loss: 0.024 (epoch loss)\n",
            "Epoch [9/10], Batch [300/375], Loss: 0.031 (epoch loss)\n",
            "Epoch [9/10], Batch [310/375], Loss: 0.033 (epoch loss)\n",
            "Epoch [9/10], Batch [320/375], Loss: 0.022 (epoch loss)\n",
            "Epoch [9/10], Batch [330/375], Loss: 0.015 (epoch loss)\n",
            "Epoch [9/10], Batch [340/375], Loss: 0.016 (epoch loss)\n",
            "Epoch [9/10], Batch [350/375], Loss: 0.006 (epoch loss)\n",
            "Epoch [9/10], Batch [360/375], Loss: 0.031 (epoch loss)\n",
            "Epoch [9/10], Batch [370/375], Loss: 0.008 (epoch loss)\n",
            "Accuracy of the network on the 24000 train images: 99 %\n",
            "Epoch [10/10], Batch [10/375], Loss: 0.015 (epoch loss)\n",
            "Epoch [10/10], Batch [20/375], Loss: 0.020 (epoch loss)\n",
            "Epoch [10/10], Batch [30/375], Loss: 0.016 (epoch loss)\n",
            "Epoch [10/10], Batch [40/375], Loss: 0.015 (epoch loss)\n",
            "Epoch [10/10], Batch [50/375], Loss: 0.010 (epoch loss)\n",
            "Epoch [10/10], Batch [60/375], Loss: 0.014 (epoch loss)\n",
            "Epoch [10/10], Batch [70/375], Loss: 0.013 (epoch loss)\n",
            "Epoch [10/10], Batch [80/375], Loss: 0.023 (epoch loss)\n",
            "Epoch [10/10], Batch [90/375], Loss: 0.007 (epoch loss)\n",
            "Epoch [10/10], Batch [100/375], Loss: 0.011 (epoch loss)\n",
            "Epoch [10/10], Batch [110/375], Loss: 0.020 (epoch loss)\n",
            "Epoch [10/10], Batch [120/375], Loss: 0.038 (epoch loss)\n",
            "Epoch [10/10], Batch [130/375], Loss: 0.039 (epoch loss)\n",
            "Epoch [10/10], Batch [140/375], Loss: 0.014 (epoch loss)\n",
            "Epoch [10/10], Batch [150/375], Loss: 0.013 (epoch loss)\n",
            "Epoch [10/10], Batch [160/375], Loss: 0.007 (epoch loss)\n",
            "Epoch [10/10], Batch [170/375], Loss: 0.005 (epoch loss)\n",
            "Epoch [10/10], Batch [180/375], Loss: 0.018 (epoch loss)\n",
            "Epoch [10/10], Batch [190/375], Loss: 0.026 (epoch loss)\n",
            "Epoch [10/10], Batch [200/375], Loss: 0.015 (epoch loss)\n",
            "Epoch [10/10], Batch [210/375], Loss: 0.030 (epoch loss)\n",
            "Epoch [10/10], Batch [220/375], Loss: 0.024 (epoch loss)\n",
            "Epoch [10/10], Batch [230/375], Loss: 0.008 (epoch loss)\n",
            "Epoch [10/10], Batch [240/375], Loss: 0.026 (epoch loss)\n",
            "Epoch [10/10], Batch [250/375], Loss: 0.004 (epoch loss)\n",
            "Epoch [10/10], Batch [260/375], Loss: 0.006 (epoch loss)\n",
            "Epoch [10/10], Batch [270/375], Loss: 0.042 (epoch loss)\n",
            "Epoch [10/10], Batch [280/375], Loss: 0.022 (epoch loss)\n",
            "Epoch [10/10], Batch [290/375], Loss: 0.017 (epoch loss)\n",
            "Epoch [10/10], Batch [300/375], Loss: 0.021 (epoch loss)\n",
            "Epoch [10/10], Batch [310/375], Loss: 0.019 (epoch loss)\n",
            "Epoch [10/10], Batch [320/375], Loss: 0.011 (epoch loss)\n",
            "Epoch [10/10], Batch [330/375], Loss: 0.010 (epoch loss)\n",
            "Epoch [10/10], Batch [340/375], Loss: 0.015 (epoch loss)\n",
            "Epoch [10/10], Batch [350/375], Loss: 0.019 (epoch loss)\n",
            "Epoch [10/10], Batch [360/375], Loss: 0.009 (epoch loss)\n",
            "Epoch [10/10], Batch [370/375], Loss: 0.011 (epoch loss)\n",
            "Accuracy of the network on the 24000 train images: 99 %\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save entire model\n",
        "torch.save(rasnet50, 'rasnet50_pytorch.pth')"
      ],
      "metadata": {
        "id": "1JrEexZa_H2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load entire model\n",
        "model = torch.load('rasnet50_pytorch.pth')"
      ],
      "metadata": {
        "id": "AKfXB_BKGCFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Assuming testloader is your DataLoader for the test dataset\n",
        "\n",
        "# Initialize variables to keep track of metrics\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "# Switch model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():  # No need to compute gradients during evaluation\n",
        "    for data in tqdm(testloader, desc=\"Testing\"):  # Loop over the test dataset\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Store predictions and true labels for metric calculation\n",
        "        output = outputs.cpu().numpy()\n",
        "        outputs = np.argmax(output,axis=1)\n",
        "        predictions.extend(outputs)\n",
        "\n",
        "        # predictions.extend()\n",
        "        # print(predictions[0])\n",
        "        true_labels.extend(labels.cpu().numpy())  #sorted\n",
        "\n",
        "\n",
        "# Calculate ROC AUC score\n",
        "roc_auc = roc_auc_score(true_labels, predictions)\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(true_labels, predicted_labels)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = (predicted_labels == true_labels).mean()\n",
        "\n",
        "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fTeZ_UhGkYs",
        "outputId": "cc63fa43-6296-4313-8beb-fe8744149814"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 94/94 [00:18<00:00,  5.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC AUC: 0.9887\n",
            "F1 Score: 0.9891\n",
            "Accuracy: 98.88%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "euv7oQ5JKvMw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}